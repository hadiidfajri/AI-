# -*- coding: utf-8 -*-
"""Fake News Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pOgSjv-7Tr8K3onwkPiFkJiGRCSO-Hr1

# Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.metrics import classification_report
import tensorflow as tf
from sklearn.metrics import confusion_matrix
from google.colab import drive
import matplotlib.pyplot as plt
from transformers import create_optimizer

"""# Data Exploration"""

from google.colab import drive
import pandas as pd

drive.mount('/content/drive')

file_path = "/content/drive/MyDrive/Dataset Sentiment/news.csv"

df = pd.read_csv(file_path)

display(df.head())

df.shape

print(df.dtypes)

print(df.duplicated().sum())

print(df.isnull().sum())

"""# Cleaning

## Data info,shape,and calculating the label

Change columns's name "unnamed" to "id"
"""

df.rename(columns={'Unnamed: 0':'id'},inplace=True)
display (df.head())

df.drop_duplicates(inplace=True)

df.shape

print((df.label == "REAL").sum())

print((df.label == "FAKE").sum())

"""## Label Encoding"""

df["label_encoding"] = LabelEncoder().fit_transform(df["label"])

"""Encoder the label to adjust the model that we're gonna use which is BERT,because Bert require the data to encode"""

df['label_encoding']

"""Fake being zero and Real being one"""

df.head()

"""## Checking Inbalanced Data"""

label_size = [df['label_encoding'].sum(),len(df['label_encoding'])-df['label_encoding'].sum()]
plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],startangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')

"""checking inbalance data,it got balance data,which is not affect to our model

# Preprocessing

## Data info (Data type,Null value,checking coloumns)
"""

df.describe()
df.info()

"""## Drop Coloumn"""

df.drop("label",axis=1,inplace=True)
  df.head()

df.drop("id",axis=1,inplace=True)
  df.head()

"""# Modelling

## Train Test Validation Data Split (Divided data into size)                   (Training : 70%,Validation : 20%,Test : 10%)
"""

df_train, df_test = train_test_split(df, test_size=0.1, random_state=3, stratify=df["label_encoding"])
df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=3, stratify=df_train["label_encoding"])

print(df_train.shape)
print(df_test.shape)
print(df_val.shape)

"""## Train Test Validation Data Split (Insert the data that it want to train,validate,and also testing)"""

x_train = ("title : " + df_train["title"] + " [SEP] " + "Content : " + df_train["text"]).tolist()
x_test = ("title : " + df_test["title"] + " [SEP] " + "Content : " + df_test["text"]).tolist()
x_val = ("title : " + df_val["title"] + " [SEP] " + "Content : " + df_val["text"]).tolist()

y_train=(df_train["label_encoding"]).tolist()
y_test=(df_test["label_encoding"]).tolist()
y_val=(df_val['label_encoding']).tolist()

"""## Loads a pretrained BERT model and tokenizer,also Prepares the model for binary text classification using TensorFlow"""

from transformers import TFBertForSequenceClassification
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)

from transformers import BertTokenizer, TFBertForSequenceClassification
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_encodings = tokenizer(x_train, truncation=True, padding=True,return_tensors="tf",max_length=128)
val_encodings = tokenizer(x_val, truncation=True, padding=True,return_tensors="tf",max_length=128)
test_encodings = tokenizer(x_test, truncation=True, padding=True,return_tensors="tf",max_length=128)

"""## Converse the label to TensorFlow Dataset Format"""

import tensorflow as tf

'''
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    tf.convert_to_tensor(y_train)
)).shuffle(1000)
#batch(4)

val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    tf.convert_to_tensor(y_val)
)).batch(4)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    tf.convert_to_tensor(y_test)
)).batch(4)
'''
batch_size = 4
epoch = 2
steps_per_epoch = int(len(y_train) / batch_size)
num_train_steps = steps_per_epoch * epoch
init_lr = 1e-5  # sama dengan 0.00001

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    tf.convert_to_tensor(y_train)
)).shuffle(1000).batch(batch_size)

val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    tf.convert_to_tensor(y_val)
)).batch(batch_size)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    tf.convert_to_tensor(y_test)
)).batch(batch_size)

"""## Training the data"""

optimizer, schedule = create_optimizer(
    init_lr=init_lr,
    num_train_steps=num_train_steps,
    num_warmup_steps=int(0.1 * num_train_steps)
)

model.compile(
    optimizer=optimizer,
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=epoch
)

"""# Reporting"""

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy over Epochs')
plt.show()

history_dict = history.history
print(history_dict.keys())

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()

plt.subplot(2, 1, 1)
plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

# Prediksi
preds = model.predict(test_dataset).logits
y_pred = np.argmax(preds, axis=1)

print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=True, yticklabels=True)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()